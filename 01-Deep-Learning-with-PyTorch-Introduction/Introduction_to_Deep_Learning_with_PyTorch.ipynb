{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **01-Introduction-to-Deep-Learning-with-PyTorch**\n"
      ],
      "metadata": {
        "id": "FvUPdwFD5WF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mohd-faizy/Developing-Large-Language-Models.git"
      ],
      "metadata": {
        "id": "l0c3u7izxW4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b75ce4-ba74-4bf6-ddda-8bcfbda20508"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Developing-Large-Language-Models'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 55 (delta 16), reused 35 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (55/55), 13.54 MiB | 17.63 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1️⃣Introduction to PyTorch, a Deep Learning Library**"
      ],
      "metadata": {
        "id": "S2qBGIWZwLyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensors: the building blocks of networks in PyTorch**"
      ],
      "metadata": {
        "id": "qooIZ8Ms3gvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from list\n",
        "import torch\n",
        "\n",
        "lst = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor = torch.tensor(lst)\n",
        "\n",
        "print(tensor)\n",
        "print(type(tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a0xQeZltI1u",
        "outputId": "1db808f1-bf4f-4a08-8f0a-d463a6bc1ada"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from NumPy array\n",
        "import numpy as np\n",
        "\n",
        "array = [[1, 2, 3], [4, 5, 6]]\n",
        "np_array = np.array(array)\n",
        "print(np_array)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "np_tensor = torch.tensor(np_array)\n",
        "print(np_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0io9Fsn1pZz",
        "outputId": "a01062ec-25d5-4aaa-b7ee-cf6ede258cfe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor attributes\n",
        "import torch\n",
        "\n",
        "lst = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor = torch.tensor(lst)\n",
        "\n",
        "print(tensor.shape)\n",
        "print(tensor.dtype)\n",
        "print(tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vJMnriP4A-I",
        "outputId": "91975d14-3bd1-4f06-f5ac-46308791b3f9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.int64\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> PyTorch doesn't guarantee the exact data type by default. It might allocate a different data type based on the hardware and software configuration. To ensure `32-bit` floats, you can explicitly specify it during creation:"
      ],
      "metadata": {
        "id": "knwSL_mJ460D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "lst_32 = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor = torch.tensor(lst_32, dtype=torch.float32)\n",
        "\n",
        "print(tensor.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn3g53pe4A7k",
        "outputId": "f5dd4ce0-19f0-4430-9ba6-8d3e97630e4d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Operations**"
      ],
      "metadata": {
        "id": "V8iLF4LV8f3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 1],\n",
        "                 [2, 2]])\n",
        "\n",
        "b = torch.tensor([[2, 2],\n",
        "                  [3, 3]])\n",
        "\n",
        "c = torch.tensor([[1, 1, 4],\n",
        "                  [2, 2, 5]])"
      ],
      "metadata": {
        "id": "ttDZj-4e54aM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2db23_486mib",
        "outputId": "bd875892-c61f-438b-93ca-c3f89d7abfab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 3],\n",
            "        [5, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a-b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWUUsF_pc_AT",
        "outputId": "bd735098-e705-400b-ff00-69e54c0a535e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1, -1],\n",
            "        [-1, -1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(a + c)\n",
        "except RuntimeError as e:\n",
        "  print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXPEq5sL68bK",
        "outputId": "4e5d4c8d-a1b6-41cd-8ca5-a140184f8a39"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a*b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1OIIzaf8QH6",
        "outputId": "58e47975-4a43-4c0a-ef57-c899c6233046"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 2],\n",
            "        [6, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network (NN) Vs. Convolutional Neural Network (CNN)**\n",
        "             \n",
        "\n",
        "\n",
        "| Feature             | Neural Network (NN)                                            | Convolutional Neural Network (CNN)                                     |\n",
        "|---------------------|-----------------------------------------------------------------|-------------------------------------------------------------------------|\n",
        "| Type                | General purpose                                                 | Subtype of NN, specialized for grid-like data (images)                  |\n",
        "| Architecture        | Fully-connected layers                                          | Convolutional layers, pooling layers, and fully-connected layers        |\n",
        "| Data Processing     | Each neuron in a layer connected to all neurons in the previous layer | Local connections between neurons, exploiting spatial relationships in data |\n",
        "| Parameter Efficiency | Less efficient, requires more parameters                       | More efficient, learns features automatically                           |\n",
        "| Strengths           | Flexible, works well for various data types                     | Excellent for image recognition, computer vision                         |\n",
        "| Applications        | Classification, regression, pattern recognition                 | Image classification, object detection, image segmentation              |\n"
      ],
      "metadata": {
        "id": "RXdvtu2sdn0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Neural Network**"
      ],
      "metadata": {
        "id": "u6XUkkHB8md6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Layer NN\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input_tensor with three features\n",
        "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
        "\n",
        "# Define our first linear layer\n",
        "# linear_layer = nn.Linear(3, 2)\n",
        "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "# Pass input through linear layer\n",
        "output = linear_layer(input_tensor)\n",
        "\n",
        "print(output)\n",
        "print(linear_layer.weight) # Each layer has a weight\n",
        "print(linear_layer.bias)   # and bias property"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlfzZoxB8via",
        "outputId": "58cb5096-0b6a-4523-bff3-23d3be460840"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4295,  0.3468]], grad_fn=<AddmmBackward0>)\n",
            "Parameter containing:\n",
            "tensor([[-0.3432,  0.3303, -0.3077],\n",
            "        [ 0.3601, -0.4120, -0.2342]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.5330,  0.3540], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`output = linear_layer(input_tensor)`\n",
        "\n",
        "for input $X$, weights $W_0$ and bias $b_0$, the linear layer performs\n",
        "\n",
        "$y_0 = W_0.X + b_0$\n",
        "\n",
        "- **In PyTorch**:\n",
        "    - Weights and bias are initialized randomly\n",
        "    - They are not useful until they are tuned\n"
      ],
      "metadata": {
        "id": "Qlkn5x_d_XIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our two-layer network summary**\n",
        "\n",
        "- Input dimensions: `1 × 3`\n",
        "- Linear layer arguments:\n",
        "    - in_features = `3`\n",
        "    - out_features = `2`\n",
        "- Output dimensions: `1 × 2`\n",
        "- Networks with only linear layers are called **fully connected**.\n",
        "- Each neuron in one layer is connected to\n",
        "each neuron in the next layer"
      ],
      "metadata": {
        "id": "-SohgFKEA8st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking layer with nn.Sequential() - Multiple Layer NN\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input_tensor with three features\n",
        "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]]) # Dim:1x3\n",
        "\n",
        "# Define the model with the first layer matching input features\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 5),\n",
        "    nn.Linear(5, 7),\n",
        "    nn.Linear(7, 5)\n",
        ")\n",
        "\n",
        "# Pass input through the model\n",
        "output_tensor = model(input_tensor)\n",
        "print(output_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yh_6vOSagOz",
        "outputId": "cfece5bb-53b8-4d19-e39b-dd770e535d80"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5969,  0.3930,  0.1064, -0.4350, -0.4013]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking layer with Activation function\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Create input_tensor with three features\n",
        "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]]) # Dim:1x3\n",
        "\n",
        "# Define the model using nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 5),    # First linear layer with 3 input features and 5 hidden units\n",
        "    nn.ReLU(),          # Activation function (ReLU in this case)\n",
        "    nn.Linear(5, 7),    # Second linear layer with 5 input features (from previous layer) and 7 hidden units\n",
        "    nn.ReLU(),          # Activation function (ReLU again)\n",
        "    nn.Linear(7, 2)     # Output layer with 7 input features (from previous layer) and 2 output units\n",
        ")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "\n",
        "# Pass input through the model\n",
        "output_tensor = model(input_tensor)\n",
        "print(output_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASh6dBTsIkRQ",
        "outputId": "bc1fa8d4-6b1f-40ba-98cb-2ae62f68d24a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=5, out_features=7, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=7, out_features=2, bias=True)\n",
            ")\n",
            "tensor([[ 0.1882, -0.1405]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discovering Activation functions**\n",
        "\n",
        "- Non-linearity allows neural networks to learn and represent complex patterns and relationships in data.\n",
        "\n",
        "- Activation functions introduce non-linearity by transforming the input signal into an output signal.\n",
        "\n",
        "- Without activation functions, neural networks would only be able to model linear relationships, limiting their capacity to learn and generalize.\n",
        "\n",
        "- Activation functions introduce important properties like boundedness and differentiability, which aid in optimization during training.\n",
        "\n",
        "- They enable neural networks to approximate any arbitrary function, making them  powerful tools for various tasks such as classification, regression, and reinforcement learning.\n",
        "\n",
        "- Common activation functions include `sigmoid`, `tanh`, `ReLU` (Rectified Linear Unit), and variants like Leaky `ReLU` and `ELU` (Exponential Linear Unit).\n",
        "\n",
        "These functions introduce complexity and flexibility, enabling neural networks to capture intricate data patterns and improve model performance."
      ],
      "metadata": {
        "id": "lwMx_IIYbBQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# Sigmoid\n",
        "\n",
        "input_tensor = torch.tensor([[0.8]])\n",
        "\n",
        "# Create a sigmoid function and apply it on input_tensor\n",
        "sigmoid = nn.Sigmoid()\n",
        "probability = sigmoid(input_tensor)\n",
        "print(probability)\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "# Softmax\n",
        "\n",
        "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
        "\n",
        "# Create a softmax function and apply it on input_tensor\n",
        "softmax = nn.Softmax(dim=-1)\n",
        "probabilities = softmax(input_tensor)\n",
        "print(probabilities)\n",
        "```"
      ],
      "metadata": {
        "id": "UEIAMbYi2sbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid**"
      ],
      "metadata": {
        "id": "BwtdhkZgrNfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the NN model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(6, 4),  # First linear layer\n",
        "    nn.Linear(4, 1),  # Second linear layer\n",
        "    nn.Sigmoid()      # Sigmoid activation function\n",
        ")\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.tensor([[6.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8lnj1D4ol9R",
        "outputId": "089f9db1-673c-4f06-ddcd-f182fef981d0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2785]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the CNN model\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 56 * 56, 128),  # 32 channels, 56x56 image size after pooling\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1),             # Output layer for binary classification\n",
        "    nn.Sigmoid()                   # Sigmoid activation function\n",
        ")\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(1, 3, 224, 224)  # Example input tensor with shape (batch_size, channels, height, width)\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z6vdZGwp6Pj",
        "outputId": "cd2360f1-8aa3-47ed-9349-5a5f683b447f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4766]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A neural network with a `single linear layer` followed by a `sigmoid` activation is similar to a **logistic regression model**.\n",
        "\n",
        "- The `input` dimension of a linear layer must be **equal** to the `output` dimension of the previous layer."
      ],
      "metadata": {
        "id": "exnJNHrY13qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax**"
      ],
      "metadata": {
        "id": "UyiZrAAOqcNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the NN model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 4),   # First linear layer\n",
        "    nn.Linear(4, 3),   # Second linear layer, output size 3 for softmax\n",
        "    nn.Softmax(dim=-1) # -1 indicates softmax is applied to the input tensor's last dimension\n",
        ")                      # nn.Softmax() can be used as last step in nn.Sequential()\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT69A1TaqgrB",
        "outputId": "c78b91ca-8646-4775-f114-53653eafc858"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0276, 0.7649, 0.2074]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2️⃣Training Our First Neural Network with PyTorch**"
      ],
      "metadata": {
        "id": "ATihhh_TzvJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is there also a backward pass?**\n",
        "\n",
        "The backward pass, also known as **backpropagation**, plays a crucial role in updating the weights and biases of a neural network during training. It is an essential step in the optimization process that allows the network to learn from the provided data.\n",
        "\n",
        "In the **training loop**, several key steps occur:\n",
        "\n",
        "1. **Forward Propagation**: Initially, data is propagated forward through the neural network. Each layer of the network applies transformations to the input data until the final output is generated.\n",
        "\n",
        "2. **Comparison to Ground-Truth**: Once the forward propagation is complete, the output produced by the network is compared to the true values, often referred to as ground-truth. This step is crucial for assessing the performance of the model.\n",
        "\n",
        "3. **Backpropagation for Weight and Bias Updates**: Following the comparison step, backpropagation is employed to update the model's weights and biases. During backpropagation, gradients are computed with respect to the loss function, and these gradients are then used to adjust the parameters of the network through techniques like gradient descent.\n",
        "\n",
        "4. **Iteration and Tuning**: The process of forward propagation, comparison, and backpropagation is repeated iteratively until the weights and biases of the model are tuned to produce useful outputs. This iterative process allows the neural network to progressively improve its performance on the given task.\n",
        "\n",
        "The backward pass is a fundamental aspect of training neural networks, enabling them to learn from data and adapt their parameters to make accurate predictions.\n",
        "\n",
        "\n",
        "**Why is Backpropagation Important?**\n",
        "\n",
        "- **Learning from Mistakes**: Backpropagation allows neural networks to learn from their mistakes by adjusting their parameters based on the errors made during prediction. By iteratively updating the weights and biases in the direction that minimizes the error, the network gradually improves its performance.\n",
        "\n",
        "- **Efficient Training**: Without backpropagation, training neural networks would be extremely challenging and inefficient. Backpropagation efficiently computes the gradients needed to update the network's parameters, making it feasible to train deep neural networks with many layers and parameters.\n",
        "\n",
        "- **Adaptability**: Neural networks trained using backpropagation can adapt to complex and non-linear relationships in the data. They can learn to recognize patterns, make predictions, and solve a wide range of tasks across various domains, including image recognition, natural language processing, and reinforcement learning.\n",
        "\n",
        "- **Generalization**: Backpropagation helps prevent overfitting by adjusting the network's parameters to generalize well to unseen data. By optimizing the loss function during training, the network learns to capture the underlying patterns in the data without memorizing noise or outliers.\n"
      ],
      "metadata": {
        "id": "O8SD03fCvoOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary classification: forward pass**"
      ],
      "metadata": {
        "id": "eUZa_NxD0rsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input data of shape 5x6 - (5 samples, 6 features)\n",
        "input_data = torch.tensor([\n",
        "     [-0.4421,  1.5207,  2.0607, -0.3647,  0.4691,  0.0946],\n",
        "     [-0.9155, -0.0475, -1.3645,  0.6336, -1.9520, -0.3398],\n",
        "     [ 0.7406,  1.6763, -0.8511,  0.2432,  0.1123, -0.0633],\n",
        "     [-1.6630, -0.0718, -0.1285,  0.5396, -0.0288, -0.8622],\n",
        "     [-0.7413,  1.7920, -0.0883, -0.6685,  0.4745, -0.4245]\n",
        "])\n",
        "\n",
        "n_classes = 1 # for Binary classif, if > 1 then Multi-class\n",
        "\n",
        "# Create binary classification model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(6, 4), # First linear layer\n",
        "    nn.Linear(4, n_classes), # Second linear layer\n",
        "    nn.Sigmoid()     # Sigmoid activation function\n",
        ")\n",
        "\n",
        "# Pass input data through model\n",
        "output = model(input_data)\n",
        "print(output.shape)\n",
        "print(output) # five probabilities between zero and one\n",
        "              # one value for each sample (row) in data"
      ],
      "metadata": {
        "id": "rg6MZxiLswqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79510e78-b99e-4a03-c793-2a0167d9b8c8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1])\n",
            "tensor([[0.4739],\n",
            "        [0.4578],\n",
            "        [0.2866],\n",
            "        [0.4935],\n",
            "        [0.3632]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-class classification: forward pass**"
      ],
      "metadata": {
        "id": "yD9AjGid0yQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input data of shape 5x6 - (5 samples, 6 features).\n",
        "input_data = torch.tensor([\n",
        "     [-0.4421,  1.5207,  2.0607, -0.3647,  0.4691,  0.0946],\n",
        "     [-0.9155, -0.0475, -1.3645,  0.6336, -1.9520, -0.3398],\n",
        "     [ 0.7406,  1.6763, -0.8511,  0.2432,  0.1123, -0.0633],\n",
        "     [-1.6630, -0.0718, -0.1285,  0.5396, -0.0288, -0.8622],\n",
        "     [-0.7413,  1.7920, -0.0883, -0.6685,  0.4745, -0.4245]\n",
        "])\n",
        "\n",
        "# Specify model has three classes\n",
        "n_classes = 3\n",
        "\n",
        "# Create binary classification model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(6, 4),         # First linear layer\n",
        "    nn.Linear(4, n_classes), # Second linear layer\n",
        "    nn.Softmax(dim=-1)\n",
        ")\n",
        "\n",
        "# Pass input data through model\n",
        "output = model(input_data)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y62JN0WKyc0S",
        "outputId": "74c45ad9-f384-43c9-e2a5-af0037012794"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[0.1129, 0.6338, 0.2533],\n",
            "        [0.1831, 0.5304, 0.2865],\n",
            "        [0.0971, 0.6695, 0.2334],\n",
            "        [0.1819, 0.5236, 0.2945],\n",
            "        [0.1013, 0.6489, 0.2498]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outputs:**\n",
        "- The output dimension is `5 × 3`\n",
        "- Each row sums to one.\n",
        "- Value with highest probability is assigned predicted label in each row.\n",
        "- Row 1 = class 1 (mammal), row 2 = class 1 (mammal), row 3 = class 3 (reptile)"
      ],
      "metadata": {
        "id": "KGgC2uo2z4R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\textbf{Note:}}$\n",
        "\n",
        "In the binary classification forward pass. if we change the second linear layer from `nn.Linear(4, 1)` to `nn.Linear(4, 3)`, we are effectively modifying it to handle multiclass classification.\n",
        "\n",
        "However, using the sigmoid activation function (`nn.Sigmoid()`) with three output units is not recommended for multiclass classification. Here’s why:\n",
        "\n",
        "- The sigmoid function produces independent probabilities for each class, but they are not guaranteed to sum up to 1.\n",
        "- It treats each class independently, which can lead to incorrect predictions when dealing with multiple classes.\n",
        "\n",
        "For multiclass tasks, it’s better to use the softmax activation function (`nn.Softmax()`), which ensures proper normalization across all classes.\n"
      ],
      "metadata": {
        "id": "lgld-e6K3bg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding | Cross Entropy in pytorch**"
      ],
      "metadata": {
        "id": "TELEkqBJ8UmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input data of shape 5x6 - (5 samples, 6 features).\n",
        "input_data = torch.tensor([\n",
        "    [-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
        "    [-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
        "    [0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n",
        "    [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
        "    [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]\n",
        "])\n",
        "\n",
        "# Specify the number of classes\n",
        "n_classes = 3\n",
        "\n",
        "# Create a binary classification model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(6, 4),  # First linear layer\n",
        "    nn.Linear(4, n_classes),  # Second linear layer\n",
        "    nn.Softmax(dim=-1)\n",
        ")\n",
        "\n",
        "# Pass input data through the model\n",
        "output = model(input_data)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output probabilities:\\n\", output)\n",
        "\n",
        "# Transform labels with one-hot encoding\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"One-hot encoding for class 0:\", F.one_hot(torch.tensor(0), num_classes=n_classes))\n",
        "print(\"One-hot encoding for class 1:\", F.one_hot(torch.tensor(1), num_classes=n_classes))\n",
        "print(\"One-hot encoding for class 2:\", F.one_hot(torch.tensor(2), num_classes=n_classes))\n",
        "\n",
        "# Compute cross-entropy loss in PyTorch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "scores = torch.tensor([[-0.1211, 0.1059]]) # Prediction\n",
        "one_hot_target = torch.tensor([[1, 0]])    # Target\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "loss = criterion(scores.double(), one_hot_target.double()) # `.double()` converts a tensor to double-precision floating-point (64-bit)\n",
        "print(\"Cross-entropy loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lKX4Qu19Mlz",
        "outputId": "7c5fbd4a-a0ef-407a-bfc6-9715c95bc36e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([5, 3])\n",
            "Output probabilities:\n",
            " tensor([[0.3142, 0.3485, 0.3373],\n",
            "        [0.3312, 0.3928, 0.2760],\n",
            "        [0.3070, 0.4371, 0.2558],\n",
            "        [0.3649, 0.2882, 0.3470],\n",
            "        [0.3153, 0.3784, 0.3063]], grad_fn=<SoftmaxBackward0>)\n",
            "One-hot encoding for class 0: tensor([1, 0, 0])\n",
            "One-hot encoding for class 1: tensor([0, 1, 0])\n",
            "One-hot encoding for class 2: tensor([0, 0, 1])\n",
            "Cross-entropy loss: tensor(0.8131, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using derivatives to update model parameters(BACKPROPAGATION)**"
      ],
      "metadata": {
        "id": "nung0wDI_zCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  # Import the optimizer module\n",
        "\n",
        "# Create the model and run a forward pass\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(16, 8),\n",
        "    nn.Linear(8, 4),\n",
        "    nn.Linear(4, 2)\n",
        ")\n",
        "sample = torch.randn(1, 16)  # Example input data\n",
        "prediction = model(sample)\n",
        "\n",
        "# Calculate the loss and compute the gradients\n",
        "target = torch.tensor([0])  # Example target (class label)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(prediction, target)\n",
        "loss.backward()\n",
        "\n",
        "# Access each layer's gradients\n",
        "print(\"Gradients for layer 0:\")\n",
        "print(\"Weight gradient:\", model[0].weight.grad)\n",
        "print(\"Bias gradient:\", model[0].bias.grad)\n",
        "\n",
        "print(\"Gradients for layer 1:\")\n",
        "print(\"Weight gradient:\", model[1].weight.grad)\n",
        "print(\"Bias gradient:\", model[1].bias.grad)\n",
        "\n",
        "print(\"Gradients for layer 2:\")\n",
        "print(\"Weight gradient:\", model[2].weight.grad)\n",
        "print(\"Bias gradient:\", model[2].bias.grad)\n",
        "\n",
        "# Introduction to deep learning with PyTorch\n",
        "# Updating model parameters\n",
        "# Update the weights by subtracting local gradients scaled by the learning rate\n",
        "lr = 0.001  # Learning rate (typically small)\n",
        "\n",
        "# Update the weights\n",
        "weight = model[0].weight\n",
        "weight_grad = model[0].weight.grad\n",
        "weight = weight - lr * weight_grad\n",
        "\n",
        "# Update the biases\n",
        "bias = model[0].bias\n",
        "bias_grad = model[0].bias.grad\n",
        "bias = bias - lr * bias_grad\n",
        "\n",
        "# Convex and non-convex functions\n",
        "# This is a convex function. This is a non-convex function.\n",
        "\n",
        "# Gradient descent\n",
        "# For non-convex functions, we will use an iterative process such as gradient descent\n",
        "# In PyTorch, an optimizer takes care of weight updates\n",
        "# The most common optimizer is stochastic gradient descent (SGD)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Optimizer handles updating model parameters (or weights) after calculation of local gradients\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK3uwU5TAPOD",
        "outputId": "6db86004-6fe8-4e1c-822f-684f0255142b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for layer 0:\n",
            "Weight gradient: tensor([[ 0.0338, -0.0135,  0.0515, -0.0179, -0.0081, -0.0350, -0.0051,  0.0138,\n",
            "         -0.0448, -0.0301,  0.0274,  0.0077, -0.0399, -0.0198,  0.0286, -0.0226],\n",
            "        [-0.0628,  0.0251, -0.0956,  0.0332,  0.0150,  0.0650,  0.0095, -0.0257,\n",
            "          0.0831,  0.0559, -0.0509, -0.0143,  0.0741,  0.0367, -0.0532,  0.0420],\n",
            "        [-0.0171,  0.0068, -0.0260,  0.0090,  0.0041,  0.0177,  0.0026, -0.0070,\n",
            "          0.0226,  0.0152, -0.0138, -0.0039,  0.0201,  0.0100, -0.0144,  0.0114],\n",
            "        [ 0.1200, -0.0479,  0.1825, -0.0633, -0.0286, -0.1241, -0.0182,  0.0491,\n",
            "         -0.1588, -0.1068,  0.0972,  0.0273, -0.1414, -0.0701,  0.1015, -0.0802],\n",
            "        [ 0.1221, -0.0488,  0.1857, -0.0645, -0.0291, -0.1263, -0.0186,  0.0499,\n",
            "         -0.1616, -0.1086,  0.0989,  0.0277, -0.1439, -0.0713,  0.1033, -0.0817],\n",
            "        [ 0.0620, -0.0247,  0.0943, -0.0327, -0.0148, -0.0641, -0.0094,  0.0253,\n",
            "         -0.0820, -0.0551,  0.0502,  0.0141, -0.0731, -0.0362,  0.0525, -0.0415],\n",
            "        [ 0.0754, -0.0301,  0.1147, -0.0398, -0.0180, -0.0780, -0.0115,  0.0308,\n",
            "         -0.0998, -0.0671,  0.0611,  0.0171, -0.0889, -0.0440,  0.0638, -0.0504],\n",
            "        [ 0.0251, -0.0100,  0.0382, -0.0133, -0.0060, -0.0260, -0.0038,  0.0103,\n",
            "         -0.0332, -0.0223,  0.0203,  0.0057, -0.0296, -0.0147,  0.0212, -0.0168]])\n",
            "Bias gradient: tensor([-0.0279,  0.0518,  0.0141, -0.0990, -0.1008, -0.0512, -0.0622, -0.0207])\n",
            "Gradients for layer 1:\n",
            "Weight gradient: tensor([[-0.0326, -0.2651,  0.0578, -0.4011, -0.1725, -0.3326,  0.1706, -0.0149],\n",
            "        [-0.0046, -0.0376,  0.0082, -0.0569, -0.0245, -0.0472,  0.0242, -0.0021],\n",
            "        [-0.0103, -0.0838,  0.0183, -0.1268, -0.0545, -0.1052,  0.0540, -0.0047],\n",
            "        [ 0.0263,  0.2137, -0.0466,  0.3234,  0.1391,  0.2682, -0.1376,  0.0120]])\n",
            "Bias gradient: tensor([ 0.4163,  0.0591,  0.1317, -0.3357])\n",
            "Gradients for layer 2:\n",
            "Weight gradient: tensor([[-0.0500, -0.3702, -0.0976,  0.1134],\n",
            "        [ 0.0500,  0.3702,  0.0976, -0.1134]])\n",
            "Bias gradient: tensor([-0.5043,  0.5043])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing our first training loop**"
      ],
      "metadata": {
        "id": "MtOJV0L-CSBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "features = diabetes.data\n",
        "target = diabetes.target\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Create a simple model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 5),  # Input layer to hidden layer\n",
        "    nn.ReLU(),         # Activation function (ReLU)\n",
        "    nn.Linear(5, 1)    # Hidden layer to output layer\n",
        ")\n",
        "\n",
        "# Create the loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)  # Lower learning rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        features, target = data        # Get feature and target from the data loader\n",
        "        target = target.unsqueeze(1)   # Unsqueeze the target tensor to match the shape expected by the loss function\n",
        "\n",
        "        optimizer.zero_grad()          # Set the gradients to zero to avoid accumulation\n",
        "        pred = model(features)         # Run a forward pass to get predictions\n",
        "        loss = criterion(pred, target) # Compute loss between predictions and actual targets\n",
        "        loss.backward()                # Compute gradients using backpropagation\n",
        "        optimizer.step()               # Update the parameters (weights and biases) of the model based on gradients\n",
        "\n",
        "# Evaluate on the entire dataset after training\n",
        "with torch.no_grad():\n",
        "    predictions = model(features)\n",
        "    loss = criterion(predictions, target)\n",
        "    print(f\"Overall Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQUkEQ9gCunV",
        "outputId": "12b8aaa7-d739-44ce-8e60-1e5cb8604cd6"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Loss: 17.7421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**:\n",
        "- **Dataset** is an abstract class representing a dataset. It allows you to access individual samples in the dataset and optionally apply transformations to the data.\n",
        "- PyTorch provides a utility class called **TensorDataset** which is a subclass of **Dataset**. It takes one or more tensors as input and combines them into a single dataset. In your code, `TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())` combines the input features and target values into a dataset.\n",
        "\n",
        "**DataLoader**:\n",
        "- **DataLoader** is responsible for creating batches of data from a dataset. It provides options for shuffling the data, specifying batch size, and parallelizing data loading.\n",
        "- By using a **DataLoader**, you can iterate over batches of data during training or evaluation, which is efficient and convenient.\n",
        "- In your code, `DataLoader(dataset, batch_size=4, shuffle=True)` creates a **DataLoader** from the dataset created earlier. It specifies a batch size of 4, meaning each iteration will provide a batch containing 4 samples. Additionally, `shuffle=True` means the data will be shuffled before each epoch to ensure randomness and better training.\n"
      ],
      "metadata": {
        "id": "NDugBTMAPMGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3️⃣Neural Network Architecture and Hyperparameters**"
      ],
      "metadata": {
        "id": "OyhwAx34z1JD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owNMnh7Msxr2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4️⃣Evaluating and Improving Models**"
      ],
      "metadata": {
        "id": "ZdqJvF5Hz5ri"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_7oEB_tAfCz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7haicxNKsyMo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $\\color{skyblue}{\\textbf{Connect with me:}}$\n",
        "\n",
        "\n",
        "[<img align=\"left\" src=\"https://cdn4.iconfinder.com/data/icons/social-media-icons-the-circle-set/48/twitter_circle-512.png\" width=\"32px\"/>][twitter]\n",
        "[<img align=\"left\" src=\"https://cdn-icons-png.flaticon.com/512/145/145807.png\" width=\"32px\"/>][linkedin]\n",
        "[<img align=\"left\" src=\"https://cdn2.iconfinder.com/data/icons/whcompare-blue-green-web-hosting-1/425/cdn-512.png\" width=\"32px\"/>][Portfolio]\n",
        "\n",
        "[twitter]: https://twitter.com/F4izy\n",
        "[linkedin]: https://www.linkedin.com/in/mohd-faizy/\n",
        "[Portfolio]: https://mohdfaizy.com/\n"
      ],
      "metadata": {
        "id": "2ZuprT2Czqry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐⭐⭐**Quick BrushUp - ML**⭐⭐⭐"
      ],
      "metadata": {
        "id": "mKs4MYM4iZIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Functions and their use cases**\n",
        "\n",
        "\n",
        "| **Activation Function** | **Formula** | **Range** | **Use Case** |\n",
        "|-------------------------|-------------|-----------|--------------|\n",
        "| Sigmoid (Logistic)      | $$f(x) = \\frac{1}{1 + e^{-x}}$$ | [0, 1] | Binary classification (e.g., spam detection) |\n",
        "| ReLU (Rectified Linear Unit) | $$f(x) = \\max(0, x)$$ | Non-negative values | Hidden layers in most neural networks |\n",
        "| Softmax                 | Given an input vector $$\\mathbf{z} = (z_1, z_2, \\ldots, z_k)$$, the softmax function computes: $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$$ | Converts raw scores into probabilities (sums to 1) | Multiclass classification (e.g., image recognition) |\n",
        "| Linear                  | $$f(x) = x$$ | Unbounded | Regression problems |\n",
        "| Tanh (Hyperbolic Tangent) | $$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ | [-1, 1] | Hidden layers in certain architectures |\n",
        "| Leaky ReLU              | $$f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{otherwise} \\end{cases}$$ | Non-negative values with a small negative slope for negative inputs | Hidden layers to prevent \"dying ReLU\" problem |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Loss Functions:**\n",
        "\n",
        "**A loss function plays a crucial role in the world of machine learning. Let’s delve into why it’s so essential:**\n",
        "\n",
        "- **Performance Measurement:** The loss function provides a clear metric to evaluate a model’s performance. It quantifies the difference between the model’s predictions and the actual target values. Essentially, it acts as a scorecard, allowing us to gauge how well the model is doing.\n",
        "\n",
        "- **Direction for Improvement:** When training a machine learning model, we want it to learn and improve iteratively. The loss function guides this improvement process. By assessing the error margin, it directs the algorithm to adjust parameters (such as weights) to minimize the loss and enhance predictions.\n",
        "\n",
        "- **Balancing Bias and Variance:** Effective loss functions help strike a balance between model bias (oversimplification) and variance (overfitting). Achieving this balance is crucial for the model’s ability to generalize well to new, unseen data.\n",
        "\n",
        "- **Influencing Model Behavior:** Different loss functions can impact the model’s behavior. Some are more robust against outliers, while others prioritize specific types of errors. Choosing the right loss function can shape how the model learns and adapts.\n",
        "\n",
        "Loss functions guide the learning process within a model, ensuring it moves in the right direction and continually improves.\n",
        "\n",
        "\n",
        "- **Classification Loss Functions**\n",
        "\n",
        "| **Loss Function** | **Formula** | **Use Case** |\n",
        "|-------------------|-------------|--------------|\n",
        "| Cross-Entropy | $$L(y, \\hat{y}) = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)$$ | - Binary Classification: Used when predicting probabilities for two classes (e.g., spam vs. not spam). - Multiclass Classification: Suitable for scenarios with more than two classes (e.g., image recognition). |\n",
        "| Hinge Loss | $$L(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})$$ | - Support Vector Machines (SVM): Used for binary classification. - Emphasizes correct classification and margin maximization. |\n",
        "| Log Loss (Logistic Loss) | $$L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})$$ | - Binary Classification: Commonly used with logistic regression. - Measures the difference between predicted probabilities and true labels. |\n",
        "| Focal Loss | $$L(y, \\hat{y}) = -(1 - \\hat{y})^\\gamma \\cdot y \\log(\\hat{y})$$ | - Imbalanced Data: Addresses class imbalance by downweighting easy examples. - Widely used in object detection tasks. |\n",
        "| Sparsemax Loss | $$L(y, \\hat{y}) = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)$$ (similar to cross-entropy) | - Multiclass Classification: Provides sparse probability distributions. - Suitable for cases where only a few classes should be activated. |\n",
        "\n",
        "- **Regression Loss Functions**\n",
        "\n",
        "| **Loss Function** | **Formula** | **Use Case** |\n",
        "|-------------------|-------------|--------------|\n",
        "| Mean Square Error (MSE) | $$L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$ | - Regression: Widely used for continuous target variables. - Measures the average squared difference between predicted and true values. |\n",
        "| Mean Absolute Error (MAE) | $$L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|$$ | - Regression: Measures average absolute difference between predicted and true values. |\n",
        "| Huber Loss | $$L(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y_i - \\hat{y}_i)^2, & \\text{if } |y_i - \\hat{y}_i| \\leq \\delta \\\\ \\delta |y_i - \\hat{y}_i| - \\frac{1}{2}\\delta^2, & \\text{otherwise} \\end{cases}$$ | - Regression: Combines MSE and MAE, robust to outliers. |\n",
        "| Quantile Loss (Pinball Loss) | $$L(y, \\hat{y}) = \\begin{cases} \\tau(y_i - \\hat{y}_i), & \\text{if } y_i \\leq \\hat{y}_i \\\\ (\\tau - 1)(y_i - \\hat{y}_i), & \\text{otherwise} \\end{cases}$$ | - Regression: Estimates conditional quantiles of the target variable. |\n",
        "| Log-Cosh Loss | $$L(y, \\hat{y}) = \\log(\\cosh(y_i - \\hat{y}_i))$$ | - Regression: Smooth approximation of MAE, robust to outliers. |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "csDDv85u1SXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of Machine Learning Tasks**\n",
        "\n",
        "- **1.Classification**\n",
        "\n",
        "    - **Classification** is the task of assigning categories (or classes) to given instances automatically. The machine learning model that has been trained to achieve such a goal is known as a **classifier**. Here are the main types of classification scenarios:\n",
        "\n",
        "    1. **Binary Classification**:\n",
        "        - An instance must belong to exactly one of two categories.\n",
        "        - Examples: Spam vs. not spam emails, disease vs. healthy patients.\n",
        "\n",
        "    2. **Multi-class Classification**:\n",
        "        - An instance must belong to exactly one of many (more than two) categories.\n",
        "        - Categories are mutually exclusive.\n",
        "        - Examples: Image recognition (identifying objects in images), natural language processing (sentiment analysis).\n",
        "\n",
        "    3. **Multi-labeled Classification**:\n",
        "        - An instance may simultaneously belong to more than one category.\n",
        "        - Categories are not mutually exclusive.\n",
        "        - Examples: Document classification (a document can be about multiple topics), tagging images with multiple labels.\n",
        "\n",
        "- **2.Regression**\n",
        "\n",
        "    - **Regression** deals with predicting continuous target variables, which represent numerical values. It aims to map input features to a continuous numerical value. Here are some regression algorithms:\n",
        "\n",
        "    1. **Linear Regression**:\n",
        "        - Fits a linear relationship between input features and the target variable.\n",
        "        - Used for predicting quantities like house prices, stock prices, etc.\n",
        "\n",
        "    2. **Polynomial Regression**:\n",
        "        - Extends linear regression by fitting higher-degree polynomial functions.\n",
        "        - Useful when the relationship between features and target is nonlinear.\n",
        "\n",
        "    3. **Ridge Regression** and **Lasso Regression**:\n",
        "        - Variants of linear regression that handle multicollinearity and overfitting.\n",
        "\n",
        "    4. **Decision Trees**:\n",
        "        - Nonlinear regression method that splits data into segments based on feature thresholds.\n",
        "        - Useful for complex relationships.\n",
        "\n",
        "\n",
        "\n",
        "- 3.**Clustering**:\n",
        "    - Clustering algorithms group similar data points together based on their features.\n",
        "    - Commonly used for customer segmentation, image segmentation, and anomaly detection.\n",
        "\n",
        "- 4.**Dimensionality Reduction**:\n",
        "    - Techniques that reduce the number of features while preserving relevant information.\n",
        "    - Examples include Principal Component Analysis (PCA) and t-SNE.\n",
        "\n",
        "- 5.**Recommendation Systems**:\n",
        "    - Used to recommend items (products, movies, etc.) to users based on their preferences and behavior.\n",
        "    - Collaborative filtering and content-based filtering are common approaches.\n",
        "\n",
        "- 6.**Time Series Forecasting**:\n",
        "    - Predicting future values based on historical data.\n",
        "    - Used in financial forecasting, weather prediction, and stock market analysis.\n",
        "\n",
        "- 7.**Natural Language Processing (NLP)**:\n",
        "    - Deals with understanding and generating human language.\n",
        "    - Sentiment analysis, machine translation, and chatbots fall under NLP.\n",
        "\n",
        "- 8.**Anomaly Detection**:\n",
        "    - Identifying rare or unusual patterns in data.\n",
        "    - Used for fraud detection, network security, and fault detection.\n",
        "\n"
      ],
      "metadata": {
        "id": "rLw-ptU62Y3c"
      }
    }
  ]
}